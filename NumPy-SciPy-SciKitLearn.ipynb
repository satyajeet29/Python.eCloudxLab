{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 2: Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# focus on lists and dictionaries as they are most frequently used datatypes. \n",
    "# NumPy utilizes \"ndarray\" object, limitation it can handle only kind of data\n",
    "# %timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import timeit\n",
    "#import pyfits\n",
    "import numpy.random as rand\n",
    "import matplotlib.pyplot as mpl\n",
    "import scipy.cluster.hierarchy as hy\n",
    "import scipy.ndimage as ndimage\n",
    "import skimage.morphology as morph\n",
    "import skimage.exposure as skie\n",
    "from skimage import filters as skie\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, geom\n",
    "from scipy.cluster import vq\n",
    "from scipy.spatial import distance\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.misc import imread, imsave\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy.linalg import eigh\n",
    "import scipy.sparse\n",
    "\n",
    "\n",
    "from scipy.optimize import curve_fit, fsolve\n",
    "from scipy.interpolate import interp1d, UnivariateSpline, griddata\n",
    "from scipy.interpolate import SmoothBivariateSpline as SBS\n",
    "from scipy.integrate import quad, trapz\n",
    "\n",
    "from mpl_toolkits.mplot3d  import Axes3D\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.datasets.samples_generator import make_regression\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array with 10^7 elements\n",
    "arr = np.arange(1e7)\n",
    "\n",
    "# converting ndarray to list\n",
    "larr = arr.tolist()\n",
    "\n",
    "# Lists by default can't boradcast so a function is coded to emulate what an ndarray can do\n",
    "def list_times(alist, scalar):\n",
    "    for i, val in enumerate(alist):\n",
    "        alist[i] = val *scalar\n",
    "    return alist\n",
    "\n",
    "# Using Python's %timeit to \n",
    "%timeit arr * 1.1\n",
    "\n",
    "# using def\n",
    "%timeit list_times(larr, 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D numpy array\n",
    "arr = np.zeros((3,3,3))\n",
    "\n",
    "# Trying to convert an array to matrix but it would fail\n",
    "try:\n",
    "    mat = np.matrix(arr)\n",
    "except ValueError as e:\n",
    "    print('Error: '+str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Array creation and data typing\n",
    "#Create a list then wrap it as np array\n",
    "alist = [1,2,3]\n",
    "arr = np.array(list)\n",
    "\n",
    "#Create an array with np elements\n",
    "arr = np.zeros(5)\n",
    "\n",
    "#Create and array going from 0 to 100\n",
    "arr = np.zeros(100)\n",
    "\n",
    "# or array from 10 to 100\n",
    "arr = np.arange(10,100)\n",
    "\n",
    "# 100 steps from 0 to 1\n",
    "arr = np.linspace(0,1,100)\n",
    "\n",
    "# logspace\n",
    "arr = np.logspace(0,1,100, base = 10.0)\n",
    "\n",
    "# Create an 5 X 5 array of zeros\n",
    "arr = np.zeros((5,5))\n",
    "\n",
    "# create a 5X5X5 cube of 1's\n",
    "cube = np.zeros((5,5,5)).astype(int) + 1\n",
    "cube = np.ones((5,5,5)).astype(np.float16)\n",
    "\n",
    "# Array of zero integers\n",
    "arr = np.zeros(2, dtype = int)\n",
    "#Array of zero floats\n",
    "arr = np.zeros(2, dtype = np.float32)\n",
    "\n",
    "#Creating an array with elements from 0 to 999\n",
    "arr1d = np.arange(1000)\n",
    "\n",
    "#Now reshaping the array\n",
    "arr3d = arr1d.reshape((10,10,10))\n",
    "\n",
    "#or\n",
    "arr3d = np.reshape(arr1d, (10,10,10))\n",
    "\n",
    "#Inversely we can flatten arrays\n",
    "arr4d = np.zeros((10,10,10,10))\n",
    "arr1d = arr4d.ravel() #flattening the array'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record Arrays\n",
    "# rec arrays\n",
    "recarr = np.zeros((2,), dtype =('i4,f4,a10'))\n",
    "\n",
    "col1 = np.arange(2) + 1\n",
    "col2 = np.arange(2, dtype =np.float32)\n",
    "col3 = ['Hello', 'World']\n",
    "\n",
    "# Here we create a list of tuples that is identical to previous add to list\n",
    "toadd = zip(col1, col2, col3)\n",
    "\n",
    "#assigning values to recarr\n",
    "recarr[:] =  toadd\n",
    "\n",
    "recarr\n",
    "\n",
    "#assigning names to each column which are now by default called  'fo', 'f1', 'f2'\n",
    "recarr.dtype.names = ('Integers','Floats', 'Strings')\n",
    "\n",
    "recarr['Integers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Indexing and slicing\n",
    "alist = [[1,2],[3,4]]\n",
    "alist[0][1]\n",
    "\n",
    "#Converting the list defined above into an array\n",
    "arr = np.array(alist)\n",
    "\n",
    "#To return (0,1) element we use...\n",
    "arr[0,1]\n",
    "\n",
    "#Now to access last column\n",
    "arr[:,1]\n",
    "\n",
    "#Accessing bottom row\n",
    "arr[1,:]\n",
    "\n",
    "#Creating an array\n",
    "arr = np.arange(5)\n",
    "\n",
    "#Creating the index array\n",
    "index = np.where(arr > 2)\n",
    "print(index)\n",
    "\n",
    "#Creating the indexarray\n",
    "new_arr =  arr[index]\n",
    "\n",
    "# remove/delete\n",
    "new_arr = np.delete(arr, index)\n",
    "\n",
    "index = arr > 2\n",
    "print(index)\n",
    "new_arr = arr[index]\n",
    "\n",
    "~ index ## inverts the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an image\n",
    "img1 = np.zeros((20,20)) + 3\n",
    "img1[4:-4,4:-4] = 6\n",
    "img1[7:-7,7:-7] = 9\n",
    "\n",
    "#filter out all values larger than 2 and less than 6\n",
    "index1 = img1 > 2\n",
    "index2 = img1 < 6\n",
    "compound_index = index1 & index2\n",
    "\n",
    "# The compound statement can alternatively be written as\n",
    "compound_index = (img1 > 3) & (img1 < 7)\n",
    "img2 = np.copy(img1)\n",
    "img2[compound_index] = 0\n",
    "\n",
    "#Making boolean arrays even more complex\n",
    "index3 = img1 == 9\n",
    "index4 = (index1 & index2) | index3\n",
    "img3 = np.copy(img1)\n",
    "img3[index4] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a 100-element array with random values from a standard nromal ditbn or a gaussian distrbn \n",
    "#sigma is 1 and mean 0\n",
    "a = rand.randn(100)\n",
    "\n",
    "index = a > 0.2\n",
    "b = a[index]\n",
    "\n",
    "b = b ** 2 - 2\n",
    "\n",
    "a[index]  =b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = np.array([[3,6,-5],[1,-3,2],[5,-1,4]])\n",
    "values = np.array([[12] ,[-2], [10]])\n",
    "\n",
    "coeff_mat = np.matrix(coeffs)\n",
    "valu_mat = np.matrix(values)\n",
    "coeff_matI = coeff_mat.getI()\n",
    "ans = np.dot(coeff_matI,valu_mat)\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= np.array([[3,6,-5],[1,-3,2],[5,-1,4]])\n",
    "b = np.array([12 ,-2, 10])\n",
    "#b= np.array([[12] ,[-2], [10]])\n",
    "\n",
    "#Solving the variables\n",
    "x = np.linalg.inv(a).dot(b) ##<<-----------------optimiized format \n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: SciPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 17 - 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Modelling and Fitting\n",
    "\n",
    "\n",
    "#Creating a function to model and create data\n",
    "def func(x,a,b):\n",
    "    return a*x + b\n",
    "\n",
    "#Generating a clean data\n",
    "x = np.linspace(0,10,100)\n",
    "y = func(x,1,2)\n",
    "\n",
    "#Adding noise to the data\n",
    "yn = y + 0.9*np.random.normal(size= len(x))\n",
    "\n",
    "#Executing the curve fit of noisy data\n",
    "popt, pcov = curve_fit(func, x, yn)\n",
    "\n",
    "#popt retunrs the best fit values for parameters of the given model (func)\n",
    "print(popt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gaussian progile, a non-linear function\n",
    "def func(x,a,b,c):\n",
    "    return a*np.exp(-(x-b)**2/(2*c**2))\n",
    "\n",
    "#Generating clean data\n",
    "x = np.linspace(0,10,100)\n",
    "y = func(x,1,5,2)\n",
    "\n",
    "#Adding noise to the data\n",
    "yn = y+0.2*np.random.normal(size = len(x))\n",
    "\n",
    "#Executing the curve fit of noisy data\n",
    "popt, pcov = curve_fit(func, x, yn)\n",
    "\n",
    "#popt retunrs the best fit values for parameters of the given model (func)\n",
    "print(popt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two Gaussian model\n",
    "def func(x,a0,b0,c0,a1,b1,c1):\n",
    "    return a0*np.exp(-(x-b0)**2/(2*c0**2))\\\n",
    "                + a1*np.exp(-(x-b1)**2/(2*c1**2))\\\n",
    "\n",
    "#Generating clean data\n",
    "x = np.linspace(0,20,200)\n",
    "y = func(x,1,3,1,-2,15,0.5)\n",
    "\n",
    "#Adding noise to the data\n",
    "yn = y+0.2*np.random.normal(size = len(x))\n",
    "\n",
    "#Since we are fitting a more complex function, providing guesses for the fitting will lead to better results\n",
    "guesses = [1,3,1,1,15,1]\n",
    "\n",
    "#Executing curve_fit on noisy data\n",
    "popt, pcov = curve_fit(func, x, yn, p0 = guesses)\n",
    "\n",
    "print(popt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to simplify intersection solution\n",
    "def findIntersection(func1, func2, x0):\n",
    "    return fsolve(lambda x: func1(x) - func2(x),  x0)\n",
    "\n",
    "#Defining functions that will intersect\n",
    "funky = lambda x : np.cos(x/5) * np.sin(x/2)\n",
    "line = lambda x : 0.01 * x  - 0.5\n",
    "\n",
    "#Defining range and getting solutions on intersection points\n",
    "x = np.linspace(0,45,10000)\n",
    "result = findIntersection(funky, line, [15,20,30,35,40,45])\n",
    "\n",
    "#Printing out results\n",
    "print(result, line(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interpolation\n",
    "#Setting up fake data\n",
    "x = np.linspace(0,10 * np.pi, 20)\n",
    "y = np.cos(x)\n",
    "\n",
    "#Interploating data\n",
    "f1 = interp1d(x, y, kind = 'linear')\n",
    "fq = interp1d(x, y, kind = 'quadratic')\n",
    "\n",
    "#x.mean and x.max are used to make sure we do not go beyond the boundaties of the data for the interpolation\n",
    "xint = np.linspace(x.min(), x.max(), 1000)\n",
    "yintl = f1(xint)\n",
    "yintq = fq(xint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 30\n",
    "x = np.linspace(1,10*np.pi,sample)\n",
    "y = np.cos(x) + np.log10(x) + np.random.randn(sample)/10\n",
    "\n",
    "#Interpolating the data\n",
    "f = UnivariateSpline(x, y, s = 1)\n",
    "\n",
    "#x.in and x.max are used to make sure we do not go beyond the boundaries of the data for the interpolation\n",
    "xint = np.linspace(x.min(), x.max(), 1000)\n",
    "yint = f(xint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple = lambda x,y: np.sqrt(x**2 + y**2) + np.sin(x**2 + y**2)\n",
    "\n",
    "#Generating griddled data \n",
    "grid_x, grid_y = np.mgrid[0:5:1000j, 0:5:100j]\n",
    "\n",
    "#Generating sample that interpolation function will see\n",
    "xy = np.random.rand(1000,2)\n",
    "sample = ripple(xy[:,0]*5, xy[:,1]*5)\n",
    "\n",
    "#Interpolating data with a cubic\n",
    "grid_z0 = griddata(xy*5, sample, (grid_x, grid_y), method = 'cubic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = xy[:,0], xy[:,1]\n",
    "sample = ripple(xy[:,0]*5, xy[:,1]*5)\n",
    "\n",
    "#Interpolating the same data generted above using SBS\n",
    "fit = SBS(x * 5, y * 5, sample, s = 0.01, kx =4, ky=4)\n",
    "interp = fit(np.linspace(0,5,1000), np.linspace(0,5,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Integration\n",
    "# Analytic Integration\n",
    "func = lambda x: np.cos(np.exp(x)) **2\n",
    "\n",
    "#Integrating function with upper and lower solution\n",
    "solution = quad(func, 0, 3)\n",
    "print(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Integration\n",
    "x = np.sort(np.random.randn(150) * 4 + 4).clip(0,5)\n",
    "func = lambda x: np.sin(x)* np.cos(x**2) + 1\n",
    "y = func(x)\n",
    "\n",
    "#Integrating the function with upper and lower limits of 0 and 5 respectively\n",
    "fsolution = quad(func, 0, 5)\n",
    "dsolution = trapz(y, x = x)\n",
    "print('fsolution = ' + str(fsolution[0]))\n",
    "print('dsolution =' + str(dsolution))\n",
    "print('The difference is '+ str(np.abs(fsolution[0] - dsolution)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statistics: \n",
    "#Construct a random array with 1000 elements\n",
    "x = np.random.randn(1000)\n",
    "#Calcluating the several of built in methods\n",
    "mean = x.mean()\n",
    "std = x.std()\n",
    "var = x.var()\n",
    "print(mean, std, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5,5,1000)\n",
    "# Here we set up parameters for the normal distribution where loc is the mean and scale is the standard deviation\n",
    "dist = norm(loc =0, scale =1)\n",
    "\n",
    "#Retrieving norms PDF and CDF\n",
    "pdf = dist.pdf(x)\n",
    "cdf = dist.cdf(x)\n",
    "\n",
    "#Here we draw out 500 random variables\n",
    "sample = dist.rvs(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.5\n",
    "dist = geom(p)\n",
    "\n",
    "#Set up simple range\n",
    "x = np.linspace(0,5,1000)\n",
    "\n",
    "#Retrieving geom's PMF & CDF\n",
    "pmf = dist.pmf(x)\n",
    "cdf = dist.cdf(x)\n",
    "\n",
    "#Here we draw out 500 random values\n",
    "sample = dist.rvs(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating a normal distribution sample with 100 elements\n",
    "sample =  np.random.randn(100)\n",
    "\n",
    "#normal tests the null hypothesis\n",
    "out = stats.normaltest(sample)\n",
    "print('normaltest output')\n",
    "print('Z-score = ' + str(out[0]))\n",
    "print('Z-score = ' + str(out[1]))\n",
    "\n",
    "#ks test is the Kolmogrov-Smirnove test for goodness of fit\n",
    "# Here the sample is being tested against the normal distribution\n",
    "# D is the KS statistinc and the closer it is to 0 the better it is\n",
    "out = stats.kstest(sample,'norm')\n",
    "print('\\nkstest output for the Normal distribution')\n",
    "print('D = '+str(out[0]))\n",
    "print('P-value = '+str(out[1]))\n",
    "\n",
    "\n",
    "#Similarly, this can be easily tested against other distributions like wald distribution\n",
    "out = stats.kstest(sample,'wald')\n",
    "print('\\nkstest output for the Wald distribution')\n",
    "print('D = '+str(out[0]))\n",
    "print('P-value = '+str(out[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = stats.hmean(sample[sample > 0])\n",
    "print(\"Harmonic mean = \"+ str(out))\n",
    "\n",
    "#This mean, where values below -1 and above 1 qre removed from mean calculation\n",
    "out = stats.tmean(sample, limits=(-1,1))\n",
    "print('\\n Trimmed mean = '+ str(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating skewness of sample\n",
    "out = stats.skew(sample)\n",
    "print('\\nSkewness = '+str(out))\n",
    "\n",
    "#Addtionally there is handy summary function called describe which gives a quick look at data\n",
    "out = stats.describe(sample)\n",
    "print('\\nSize = ' + str(out[0]))\n",
    "print('Min = '+ str(out[1][0]))\n",
    "print('Max = '+ str(out[1][1]))\n",
    "print('Mean = '+ str(out[2]))\n",
    "print('Variance = '+ str(out[3]))\n",
    "print('Skewness = '+ str(out[4]))\n",
    "print('Skewness = '+ str(out[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spatial and Clustering Analysis: useful for organizing discovered information\n",
    "# vector quantization, hierarchical clustering\n",
    "#Creating data:\n",
    "c1 = np.random.randn(100,2) + 5\n",
    "c2 = np.random.randn(30,2) - 5\n",
    "c3 = np.random.randn(50,2)\n",
    "\n",
    "#pooling all the data into one 180X2 array\n",
    "data = np.vstack([c1,c2,c3])\n",
    "\n",
    "#Calculating the cluster centroids and variance from kmeans\n",
    "centroids, variance = vq.kmeans(data,3)\n",
    "\n",
    "#The identified variable contains the information we need to separate the points in clusters based on vq function\n",
    "identified, distance = vq.vq(data, centroids)\n",
    "\n",
    "#Retrieving coordinates for points in each vq identified core\n",
    "vqc1 = data[identified ==0]\n",
    "vqc2 = data[identified ==2]\n",
    "vqc3 = data[identified ==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hierarchical Clustering\n",
    "# Creating a cluster of clusters function\n",
    "def clusters(number = 20, cnumber = 5, csize = 10):\n",
    "    #Note that the way the clusters are positioned in Gaussian randomness:\n",
    "    rnum = np.random.rand(cnumber,2)\n",
    "    rn = rnum[:,0]*number\n",
    "    rn = rn.astype(int)\n",
    "    rn[np.where(rn < 5)] = 5\n",
    "    rn[np.where(rn > number/2.)] = round(number / 2., 0)\n",
    "    ra = rnum[:,1]*2.9\n",
    "    ra[np.where(ra < 1.5)] = 1.5\n",
    "    cls = np.random.randn(number, 3)*csize\n",
    "    #Random multipliers for central point of cluster\n",
    "    rxyz = np.random.randn(number-1,3)\n",
    "    for i in xrange(cnumber, -1):\n",
    "        tmp = np.randomrandn(rn[i+1],3)\n",
    "        x = tmp[:,0] + (rxyz[i,0]*csize)\n",
    "        y = tmp[:,0] + (rxyz[i,1]*csize)\n",
    "        y = tmp[:,0] + (rxyz[i,2]*csize)\n",
    "        tmp = np.column_stack([x,y,z])\n",
    "        cls = np.vstack([cls,tmp])\n",
    "    return cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate  cluster of clusters\n",
    "cls = clusters()\n",
    "D = pdist(cls[:,0:2])\n",
    "D = squareform(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute and plot first dendogram.\n",
    "fig = mpl.figure(figsize=(8,8))\n",
    "ax1 = fig.add_axes([0.09,0.1,0.2,0.6])\n",
    "Y1 = hy.linkage(D, method = 'complete')\n",
    "cutoff = 0.3 * np.max(Y1[:,2])\n",
    "Z1 = hy.dendrogram(Y1, orientation = 'right', color_threshold = cutoff)\n",
    "ax1.xaxis.set_visible(False)\n",
    "ax1.yaxis.set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2 = fig.add_axes([0.3,0.71,0.6,0.2])\n",
    "Y2 = hy.linkage(D, method = 'average')\n",
    "cutoff = 0.3 * np.max(Y2[:,2])\n",
    "Z2 = hy.dendrogram(Y2, color_threshold = cutoff)\n",
    "ax1.xaxis.set_visible(False)\n",
    "ax1.yaxis.set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot distance matrix\n",
    "ax3 = fig.add_axes([0.3,0.1,0.6,0.6])\n",
    "idx1 = Z1['leaves']\n",
    "idx2 = Z2['leaves']\n",
    "D = D[idx1,:]\n",
    "D = D[:,idx2]\n",
    "ax3.matshow(D, aspect = 'auto', origin = 'lower', cmap = mpl.cm.YlGnBu)\n",
    "ax3.xaxis.set_visible(False)\n",
    "ax3.yaxis.set_visible(False)\n",
    "\n",
    "#Plot colorbar\n",
    "fig.savefig(\"cluster_hy_01.pdf\", bbox = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save imports and cluster fruntion from previos example\n",
    "\n",
    "def group(data, index):\n",
    "    number = np.unique(index)\n",
    "    groups = []\n",
    "    for i in number:\n",
    "        groups.append(data[index==i])\n",
    "    return groups\n",
    "\n",
    "# Creating a cluster of clusters\n",
    "cls = clusters()\n",
    "\n",
    "#calculating the linkage matrix\n",
    "Y = hy.linkage(cls[:,0:2], method ='complete')\n",
    "\n",
    "#Here we use the fclsuter function to pull out a collection of flat clusters from the hierarchical dats structure.\n",
    "#Note that we are using the same cutoff values as in the previous example for the dendogram using the 'complete' method\n",
    "\n",
    "cutoff = 0.3 * np.max(Y[:,2])\n",
    "index = hy.fcluster(Y,cutoff,'distance')\n",
    "\n",
    "groups = group(cls, index)\n",
    "\n",
    "#Plotting clusters\n",
    "fig = mpl.figure(figsize =(6,6))\n",
    "ax = fig.add_subplot(111)\n",
    "colors = ['r','c','b','g','orange','k','y','gray']\n",
    "for i, g in enumerate(groups):\n",
    "    i = np.mod(i, len(colors))\n",
    "    ax.scatter(g[:,0], g[:,1], c= colors[i], edgecolor = 'none', s =50)\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.yaxis.set_visible(False)\n",
    "\n",
    "    fig.savefig(\"cluster_hy_02.pdf\", bbox = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Signal and Image processing:\n",
    "# Getting the list of files in the directory\n",
    "files = glob('imagery/WestAfrica-Alps_2017-12_imagery/**.jpg')\n",
    "# Opening up the first image for loop\n",
    "im1 = imread(files[0]).astype(np.float32)\n",
    "# Starting loop and continue co-adding new images\n",
    "for i in xrange(1, len(files)):\n",
    "    print i\n",
    "    im1 += imread(files[i]).astype(np.float32)\n",
    "# Saving img\n",
    "imsave('stacked_image.jpg', im1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function allows us to place in the\n",
    "# brightest pixels per x and y position between\n",
    "# two images. It is similar to PIL's\n",
    "# ImageChop.Lighter function.\n",
    "def chop_lighter(image1, image2):\n",
    "    s1 = np.sum(image1, axis=2)\n",
    "    s2 = np.sum(image2, axis=2)\n",
    "    \n",
    "    index = s1 < s2\n",
    "    image1[index, 0] = image2[index, 0]\n",
    "    image1[index, 1] = image2[index, 1]\n",
    "    image1[index, 2] = image2[index, 2]\n",
    "    return image1\n",
    "\n",
    "# Getting the list of files in the directory\n",
    "#files = glob('space/*.JPG')\n",
    "# Opening up the first image for looping\n",
    "im1 = imread(files[0]).astype(np.float32)\n",
    "im2 = np.copy(im1)\n",
    "\n",
    "# Starting loop\n",
    "for i in xrange(1, len(files)):\n",
    "    print i\n",
    "    im = imread(files[i]).astype(np.float32)\n",
    "    # Same before\n",
    "    im1 += im\n",
    "    # im2 shows star trails better\n",
    "    im2 = chop_lighter(im2, im)\n",
    "\n",
    "# Saving image with slight tweaking on the combination\n",
    "# of the two images to show star trails with the\n",
    "# co-added image.\n",
    "imsave('stacked_image_r2.jpg', im1/im1.max() + im2/im2.max()*0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sparse Matrices\n",
    "N = 3000\n",
    "# Creating a random sparse matrix\n",
    "m = scipy.sparse.rand(N, N)\n",
    "# Creating an array clone of it\n",
    "a = m.toarray()\n",
    "print('The numpy array data size: ' + str(a.nbytes) + ' bytes')\n",
    "print('The sparse matrix data size: ' + str(m.data.nbytes) + ' bytes')\n",
    "# Non-sparse\n",
    "t0 = time.time()\n",
    "\n",
    "res1 = eigh(a)\n",
    "dt = str(np.round(time.time() - t0, 3)) + ' seconds'\n",
    "print('Non-sparse operation takes ' + dt)\n",
    "# Sparse\n",
    "t0 = time.time()\n",
    "res2 = eigsh(m)\n",
    "dt = str(np.round(time.time() - t0, 3)) + ' seconds'\n",
    "print('Sparse operation takes ' + dt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciKit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 4: SciKit: Taking Scipy One Step Further "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Page 43 - 53\n",
    "## Scikit-Image\n",
    "#  Scikit-image has fortunately taken on\n",
    "#  the task of going a step further to provide more advanced functions that we may\n",
    "#  need for scientific research. These advanced and high-level modules include color\n",
    "#  space conversion, image intensity adjustment algorithms, feature detections, filters for\n",
    "#  sharpening and denoising, read/write capabilities, and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating data points with a non-uniform background\n",
    "x = np.random.uniform(low=0, high=100, size=20).astype(int)\n",
    "y = np.random.uniform(low=0, high=100, size=20).astype(int)\n",
    "# Creating image with non-uniform background\n",
    "func = lambda x, y: x**2 + y**2\n",
    "grid_x, grid_y = np.mgrid[-1:1:100j, -2:2:100j]\n",
    "bkg = func(grid_x, grid_y)\n",
    "bkg = bkg / np.max(bkg)\n",
    "# Creating points\n",
    "clean = np.zeros((100,100))\n",
    "clean[(x,y)] += 5\n",
    "clean = ndimage.gaussian_filter(clean, 3)\n",
    "clean = clean / np.max(clean)\n",
    "# Combining both the non-uniform background\n",
    "# and points\n",
    "fimg = bkg + clean\n",
    "fimg = fimg / np.max(fimg)\n",
    "# Defining minimum neighboring size of objects\n",
    "block_size = 3\n",
    "# Adaptive threshold function which returns image\n",
    "# map of structures that are different relative to\n",
    "# background\n",
    "adaptive_cut = skif.threshold_adaptive(fimg, block_size, offset=0)\n",
    "\n",
    "# Global threshold\n",
    "global_thresh = skif.threshold_otsu(fimg)\n",
    "global_cut = fimg > global_thresh\n",
    "# Creating figure to highlight difference between\n",
    "# adaptive and global threshold methods\n",
    "fig = mpl.figure(figsize=(8, 4))\n",
    "fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax1.imshow(fimg)\n",
    "ax1.xaxis.set_visible(False)\n",
    "ax1.yaxis.set_visible(False)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax2.imshow(global_cut)\n",
    "ax2.xaxis.set_visible(False)\n",
    "ax2.yaxis.set_visible(False)\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax3.imshow(adaptive_cut)\n",
    "ax3.xaxis.set_visible(False)\n",
    "ax3.yaxis.set_visible(False)\n",
    "fig.savefig('scikit_image_f01.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Local Maxima Page 45\n",
    "# Generating data points with a non-uniform background\n",
    "x = np.random.uniform(low=0, high=200, size=20).astype(int)\n",
    "y = np.random.uniform(low=0, high=400, size=20).astype(int)\n",
    "# Creating image with non-uniform background\n",
    "func = lambda x, y: np.cos(x)+ np.sin(y)\n",
    "grid_x, grid_y = np.mgrid[0:12:200j, 0:24:400j]\n",
    "bkg = func(grid_x, grid_y)\n",
    "bkg = bkg / np.max(bkg)\n",
    "\n",
    "\n",
    "# Creating points\n",
    "clean = np.zeros((200,400))\n",
    "clean[(x,y)] += 5\n",
    "clean = ndimage.gaussian_filter(clean, 3)\n",
    "clean = clean / np.max(clean)\n",
    "# Combining both the non-uniform background\n",
    "# and points\n",
    "fimg = bkg + clean\n",
    "fimg = fimg / np.max(fimg)\n",
    "# Calculating local maxima\n",
    "lm1 = morph.local_maxima(fimg)\n",
    "x1, y1 = np.where(lm1.T == True)\n",
    "# Creating figure to show local maximum detection\n",
    "# rate success\n",
    "fig = mpl.figure(figsize=(8, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(fimg)\n",
    "ax.scatter(x1, y1, s=100, facecolor='none', edgecolor='#009999')\n",
    "ax.set_xlim(0,400)\n",
    "ax.set_ylim(0,200)\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.yaxis.set_visible(False)\n",
    "fig.savefig('scikit_image_f02.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading astronomy image from an infrared space telescope\n",
    "img = pyfits.getdata('stellar_cluster.fits')[500:1500, 500:1500]\n",
    "# Prep file scikit-image environment and plotting\n",
    "limg = np.arcsinh(img)\n",
    "limg = limg / limg.max()\n",
    "low = np.percentile(limg, 0.25)\n",
    "high = np.percentile(limg, 99.5)\n",
    "opt_img = skie.exposure.rescale_intensity(limg, in_range=(low, high))\n",
    "# Calculating local maxima and filtering out noise\n",
    "lm = morph.is_local_maximum(limg)\n",
    "x1, y1 = np.where(lm.T == True)\n",
    "v = limg[(y1, x1)]\n",
    "lim = 0.5\n",
    "x2, y2 = x1[v > lim], y1[v > lim]\n",
    "# Creating figure to show local maximum detection\n",
    "# rate success\n",
    "fig = mpl.figure(figsize=(8,4))\n",
    "fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.imshow(opt_img)\n",
    "ax1.set_xlim(0, img.shape[1])\n",
    "ax1.set_ylim(0, img.shape[0])\n",
    "ax1.xaxis.set_visible(False)\n",
    "ax1.yaxis.set_visible(False)\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.imshow(opt_img)\n",
    "ax2.scatter(x2, y2, s=80, facecolor='none', edgecolor='#FF7400')\n",
    "ax2.set_xlim(0, img.shape[1])\n",
    "ax2.set_ylim(0, img.shape[0])\n",
    "ax2.xaxis.set_visible(False)\n",
    "ax2.yaxis.set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-Learn\n",
    "# Linear Regression\n",
    "# Generating synthetic data for training and testing\n",
    "X, y = make_regression(n_samples=100, n_features=2, n_informative=1,\\\n",
    "random_state=0, noise=50)\n",
    "# X and y are values for 3D space. We first need to train\n",
    "# the machine, so we split X and y into X_train, X_test,\n",
    "# y_train, and y_test. The *_train data will be given to the\n",
    "# model to train it.\n",
    "X_train, X_test = X[:80], X[-20:]\n",
    "y_train, y_test = y[:80], y[-20:]\n",
    "\n",
    "# Creating instance of model\n",
    "regr = linear_model.LinearRegression()\n",
    "# Training the model\n",
    "regr.fit(X_train, y_train)\n",
    "# Printing the coefficients\n",
    "print(regr.coef_)\n",
    "# [-10.25691752 90.5463984 ]\n",
    "# Predicting y-value based on training\n",
    "X11 = np.array([1.2, 4])\n",
    "X1 = X11.reshape(1,-1) # <<-- correction step\n",
    "print(regr.predict(X1))\n",
    "# 350.860363861\n",
    "# With the *_test data we can see how the result matches\n",
    "# the data the model was trained with.\n",
    "# It should be a good match as the *_train and *_test\n",
    "# data come from the same sample. Output: 1 is perfect\n",
    "# prediction and anything lower is worse.\n",
    "print(regr.score(X_test, y_test))\n",
    "# 0.949827492261\n",
    "fig = mpl.figure(figsize=(8, 5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "# ax = Axes3D(fig)\n",
    "# Data\n",
    "ax.scatter(X_train[:,0], X_train[:,1], y_train, facecolor='#00CC00')\n",
    "ax.scatter(X_test[:,0], X_test[:,1], y_test, facecolor='#FF7800')\n",
    "# Function with coefficient variables\n",
    "coef = regr.coef_\n",
    "line = lambda x1, x2: coef[0] * x1 + coef[1] * x2\n",
    "grid_x1, grid_x2 = np.mgrid[-2:2:10j, -2:2:10j]\n",
    "ax.plot_surface(grid_x1, grid_x2, line(grid_x1, grid_x2),\n",
    "alpha=0.1, color='k')\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.yaxis.set_visible(False)\n",
    "ax.zaxis.set_visible(False)\n",
    "fig.savefig('scikit_learn_regression.pdf', bbox='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() got an unexpected keyword argument 'eps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-000ee9e8e7d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# db.labels_ is an array with identifiers to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# different clusters in the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDBSCAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Retrieving coordinates for points in each\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: fit() got an unexpected keyword argument 'eps'"
     ]
    }
   ],
   "source": [
    "# Clustering:\n",
    "# Creating data\n",
    "c1 = np.random.randn(100, 2) + 5\n",
    "c2 = np.random.randn(50, 2)\n",
    "# Creating a uniformly distributed background\n",
    "u1 = np.random.uniform(low=-10, high=10, size=100)\n",
    "u2 = np.random.uniform(low=-10, high=10, size=100)\n",
    "c3 = np.column_stack([u1, u2])\n",
    "# Pooling all the data into one 150 x 2 array\n",
    "data = np.vstack([c1, c2, c3])\n",
    "# Calculating the cluster with DBSCAN function.\n",
    "# db.labels_ is an array with identifiers to the\n",
    "# different clusters in the data.\n",
    "db = DBSCAN().fit(data, eps=0.95, min_samples=10)\n",
    "labels = db.labels_\n",
    "# Retrieving coordinates for points in each\n",
    "# identified core. There are two clusters\n",
    "# denoted as 0 and 1 and the noise is denoted\n",
    "# as -1. Here we split the data based on which\n",
    "# component they belong to.\n",
    "dbc1 = data[labels == 0]\n",
    "dbc2 = data[labels == 1]\n",
    "noise = data[labels == -1]\n",
    "# Setting up plot details\n",
    "x1, x2 = -12, 12\n",
    "y1, y2 = -12, 12\n",
    "fig = mpl.figure()\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "ax1 = fig.add_subplot(121, aspect='equal')\n",
    "ax1.scatter(c1[:,0], c1[:,1], lw=0.5, color='#00CC00')\n",
    "ax1.scatter(c2[:,0], c2[:,1], lw=0.5, color='#028E9B')\n",
    "ax1.scatter(c3[:,0], c3[:,1], lw=0.5, color='#FF7800')\n",
    "ax1.xaxis.set_visible(False)\n",
    "ax1.yaxis.set_visible(False)\n",
    "ax1.set_xlim(x1, x2)\n",
    "ax1.set_ylim(y1, y2)\n",
    "ax1.text(-11, 10, 'Original')\n",
    "ax2 = fig.add_subplot(122, aspect='equal')\n",
    "ax2.scatter(dbc1[:,0], dbc1[:,1], lw=0.5, color='#00CC00')\n",
    "ax2.scatter(dbc2[:,0], dbc2[:,1], lw=0.5, color='#028E9B')\n",
    "ax2.scatter(noise[:,0], noise[:,1], lw=0.5, color='#FF7800')\n",
    "ax2.xaxis.set_visible(False)\n",
    "ax2.yaxis.set_visible(False)\n",
    "ax2.set_xlim(x1, x2)\n",
    "ax2.set_ylim(y1, y2)\n",
    "ax2.text(-11, 10, 'DBSCAN identified')\n",
    "fig.savefig('scikit_learn_clusters.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBSCAN().fit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
